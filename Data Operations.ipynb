{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5670349b",
   "metadata": {},
   "source": [
    "# NumPy\n",
    "* **Numpy Stands for Numerical Python**\n",
    "* **Numpy is increadiable Library to perform Mathematical & Statistical Opertation**\n",
    "* **It Is Memory and work Efficient**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7aabf",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e573b314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [2.66453526e-15 2.00000000e+00 3.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "X_b = np.c_[np.ones((len(X), 1)), X]              #X_b - adding baised to the linear term\n",
    "\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)         #Linalg-linear algo, Î¸ = (X^T * X)^(-1) * X^T * y\n",
    "\n",
    "X_new = np.array([[0], [2], [3]])\n",
    "X_new_b = np.c_[np.ones((len(X_new), 1)), X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "\n",
    "print(\"Predictions:\", y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e841e",
   "metadata": {},
   "source": [
    "## Logictic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9e87ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: [[-6.01274279]\n",
      " [ 2.50992492]]\n",
      "Predictions: [[0.02923216]\n",
      " [0.82010101]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def gradient_descent(X, y, learning_rate, iterations):\n",
    "    m = len(y)\n",
    "    X_b = np.c_[np.ones((m, 1)), X]\n",
    "    theta = np.random.randn(X_b.shape[1], 1)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        z = X_b.dot(theta)\n",
    "        predictions = sigmoid(z)\n",
    "        gradients = X_b.T.dot(predictions - y) / m\n",
    "        theta -= learning_rate * gradients\n",
    "    \n",
    "    return theta\n",
    "\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([[0], [0], [1], [1]])\n",
    "\n",
    "theta_best = gradient_descent(X, y, learning_rate=0.1, iterations=1000)\n",
    "print(\"Best parameters:\", theta_best)\n",
    "\n",
    "X_new = np.array([[1], [3]])\n",
    "X_new_b = np.c_[np.ones((len(X_new), 1)), X_new]\n",
    "predictions = sigmoid(X_new_b.dot(theta_best))\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6cebb6",
   "metadata": {},
   "source": [
    "## KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc469700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Distance function\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b)**2))\n",
    "\n",
    "# K-NN classifier\n",
    "def knn(X_train, y_train, X_test, k=3):\n",
    "    y_pred = []\n",
    "    for test_point in X_test:\n",
    "        distances = [euclidean_distance(test_point, x) for x in X_train]\n",
    "        k_indices = np.argsort(distances)[:k]\n",
    "        k_nearest_labels = [y_train[i] for i in k_indices]\n",
    "        majority_vote = Counter(k_nearest_labels).most_common(1)[0][0]\n",
    "        y_pred.append(majority_vote)\n",
    "    return y_pred\n",
    "\n",
    "# Sample data\n",
    "X_train = np.array([[1, 1], [2, 2], [3, 3], [6, 6], [7, 7]])\n",
    "y_train = np.array([0, 0, 0, 1, 1])\n",
    "X_test = np.array([[4, 4], [5, 5]])\n",
    "\n",
    "# Run K-NN classifier\n",
    "y_pred = knn(X_train, y_train, X_test, k=3)\n",
    "print(\"Predicted labels:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9326f3",
   "metadata": {},
   "source": [
    "## New Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c14354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA transformed data:\n",
      " [[ 0.44362444 -0.20099093]\n",
      " [-2.17719404 -0.05500992]\n",
      " [ 0.57071239  0.36808609]\n",
      " [-0.12902465  0.06747325]\n",
      " [ 1.29188186 -0.17955849]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data (each row is a data point)\n",
    "X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0]])\n",
    "\n",
    "# Standardize data (zero mean and unit variance)\n",
    "X_meaned = X - np.mean(X, axis=0)\n",
    "\n",
    "# Covariance matrix\n",
    "cov_matrix = np.cov(X_meaned.T)\n",
    "\n",
    "# Eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort eigenvectors by eigenvalues in decreasing order\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "# Transform the data to the new basis\n",
    "X_pca = X_meaned.dot(eigenvectors)\n",
    "\n",
    "print(\"PCA transformed data:\\n\", X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f02a35",
   "metadata": {},
   "source": [
    "## K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a862edbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids:\n",
      " [[7.33333333 9.        ]\n",
      " [1.16666667 1.46666667]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Euclidean distance\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b)**2))\n",
    "\n",
    "# K-Means algorithm\n",
    "def kmeans(X, k, iterations):\n",
    "    # Randomly initialize centroids\n",
    "    centroids = X[np.random.choice(X.shape[0], k, replace=False)]\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # Assign points to the nearest centroid\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        for point in X:\n",
    "            distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
    "            cluster_index = np.argmin(distances)\n",
    "            clusters[cluster_index].append(point)\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids = np.array([np.mean(cluster, axis=0) for cluster in clusters])\n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    return centroids, clusters\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n",
    "\n",
    "# Run K-Means\n",
    "centroids, clusters = kmeans(X, k=2, iterations=100)\n",
    "print(\"Centroids:\\n\", centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b35af4",
   "metadata": {},
   "source": [
    "## Run Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7680b892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate probabilities\n",
    "def calculate_probabilities(X, y):\n",
    "    unique_classes = np.unique(y)\n",
    "    probabilities = {}\n",
    "    \n",
    "    for label in unique_classes:\n",
    "        X_class = X[y == label]\n",
    "        probabilities[label] = {}\n",
    "        probabilities[label]['prior'] = len(X_class) / len(y)\n",
    "        probabilities[label]['mean'] = np.mean(X_class, axis=0)\n",
    "        probabilities[label]['var'] = np.var(X_class, axis=0)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "# Gaussian probability\n",
    "def gaussian_probability(x, mean, var):\n",
    "    coeff = 1 / np.sqrt(2 * np.pi * var)\n",
    "    exponent = np.exp(-(x - mean)**2 / (2 * var))\n",
    "    return coeff * exponent\n",
    "\n",
    "# Predict class label\n",
    "def predict(X_test, probabilities):\n",
    "    predictions = []\n",
    "    for x in X_test:\n",
    "        posteriors = {}\n",
    "        for label, params in probabilities.items():\n",
    "            prior = np.log(params['prior'])\n",
    "            likelihood = np.sum(np.log(gaussian_probability(x, params['mean'], params['var'])))\n",
    "            posteriors[label] = prior + likelihood\n",
    "        predictions.append(max(posteriors, key=posteriors.get))\n",
    "    return predictions\n",
    "\n",
    "# Sample data\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y_train = np.array([0, 0, 1, 1, 1])\n",
    "X_test = np.array([[2.5, 3.5], [3.5, 4.5]])\n",
    "\n",
    "# Run Naive Bayes classifier\n",
    "probabilities = calculate_probabilities(X_train, y_train)\n",
    "y_pred = predict(X_test, probabilities)\n",
    "print(\"Predictions:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303e437",
   "metadata": {},
   "source": [
    "## Support vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74e64e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight vector: [-0.72999255  0.57392065]\n",
      "Bias term: -0.9520000000000007\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Hinge loss function for SVM\n",
    "def hinge_loss(X, y, w, b, reg_strength):\n",
    "    n_samples = X.shape[0]\n",
    "    distances = 1 - y * (np.dot(X, w) + b)\n",
    "    losses = np.maximum(0, distances)\n",
    "    return reg_strength * (np.sum(losses) / n_samples)\n",
    "\n",
    "# Gradient descent for SVM\n",
    "def gradient_descent(X, y, w, b, learning_rate, iterations, reg_strength):\n",
    "    for _ in range(iterations):\n",
    "        for i, x_i in enumerate(X):\n",
    "            condition = y[i] * (np.dot(X[i], w) + b) >= 1\n",
    "            if condition:\n",
    "                w -= learning_rate * (2 * reg_strength * w)\n",
    "            else:\n",
    "                w -= learning_rate * (2 * reg_strength * w - np.dot(X[i], y[i]))\n",
    "                b -= learning_rate * y[i]\n",
    "    return w, b\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [2, 3], [3, 3], [4, 5], [5, 6]])\n",
    "y = np.array([1, 1, -1, -1, -1])\n",
    "\n",
    "# Parameters\n",
    "w = np.zeros(X.shape[1])\n",
    "b = 0\n",
    "learning_rate = 0.001\n",
    "iterations = 1000\n",
    "reg_strength = 0.01\n",
    "\n",
    "# Train SVM\n",
    "w, b = gradient_descent(X, y, w, b, learning_rate, iterations, reg_strength)\n",
    "print(\"Weight vector:\", w)\n",
    "print(\"Bias term:\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96d496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
